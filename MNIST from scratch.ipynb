{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d043a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68969a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "\n",
    "def read_mnist_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num_images = struct.unpack('>II', f.read(8))\n",
    "        rows, cols = struct.unpack('>II', f.read(8))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows * cols)\n",
    "    return images\n",
    "\n",
    "def read_mnist_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Replace these paths with your actual file paths\n",
    "train_images_path = 'Dataset/train-images-idx3-ubyte/train-images-idx3-ubyte'  # Update with your path\n",
    "train_labels_path = 'Dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte'  # Update with your path\n",
    "test_images_path = 't10k-images-idx3-ubyte'    # Update with your path\n",
    "test_labels_path = 't10k-labels-idx1-ubyte'    # Update with your path\n",
    "\n",
    "# Load data\n",
    "train_images = read_mnist_images(train_images_path)\n",
    "train_labels = read_mnist_labels(train_labels_path)\n",
    "\n",
    "# Create DataFrames\n",
    "# First for the features (images)\n",
    "train_df = pd.DataFrame(train_images)\n",
    "# Add the labels\n",
    "train_df['label'] = train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f38e8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  775  776  777  778  779  780  781  782  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   783  label  \n",
       "0    0      5  \n",
       "1    0      0  \n",
       "2    0      4  \n",
       "3    0      1  \n",
       "4    0      9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=train_df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16ff35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.values\n",
    "np.random.shuffle(data)\n",
    "num_samples=data.shape[0]\n",
    "train_end = int(0.7 * num_samples)\n",
    "val_end= int(0.85*num_samples)\n",
    "train_data=data[:train_end]\n",
    "val_data=data[train_end:val_end]\n",
    "test_data=data[val_end:]\n",
    "\n",
    "X_train=train_data[:,:-1]\n",
    "y_train=train_data[:,-1]\n",
    "X_val=val_data[:,:-1]\n",
    "y_val=val_data[:,-1]\n",
    "X_test=test_data[:,:-1]\n",
    "y_test=test_data[:,-1]\n",
    "X_train=X_train/255.0\n",
    "X_val=X_val/255.0\n",
    "X_test=X_test/255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96d8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neuron):\n",
    "        self.weights=0.10*np.random.randn(n_inputs,n_neuron)\n",
    "        self.bias=0.10*np.random.randn(1,n_neuron)\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.dot(inputs,self.weights)+self.bias\n",
    "    def backward(self,dvalues):\n",
    "        self.dweights=np.dot(self.inputs.T,dvalues)\n",
    "        self.dbias=np.sum(dvalues,axis=0,keepdims=True)\n",
    "        self.dinputs=np.dot(dvalues,self.weights.T)\n",
    "\n",
    "class Activation_RelU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.maximum(0,inputs)\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=dvalues.copy()\n",
    "        self.dinputs[self.inputs<=0]=0\n",
    "\n",
    "class Activation_SoftMax:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        probablities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probablities\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses=self.forward(output,y)\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss\n",
    "class Loss_CategoricalCross_Entropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples=len(y_pred)\n",
    "        y_pred_clipped=np.clip(y_pred,1e-7,1-1e-7)\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidence=y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidence=np.sum(y_pred_clipped*y_true,axis=1)\n",
    "        negative_log_likelihood=-np.log(correct_confidence)\n",
    "        return negative_log_likelihood\n",
    "\n",
    "class Activation_SoftMax_Loss_Catorgorical_Cross_Entropy():\n",
    "    def __init__(self):\n",
    "        self.activation=Activation_SoftMax()\n",
    "        self.loss=Loss_CategoricalCross_Entropy()\n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output=self.activation.output\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples=len(dvalues)\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true=np.argmax(y_true,axis=1)\n",
    "        self.dinputs=dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -=1\n",
    "        self.dinputs=self.dinputs/samples\n",
    "    \n",
    "class Optimizer_GD:\n",
    "    def __init__(self,learning_rate=1):\n",
    "        self.learning_rate=learning_rate\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate*layer.dweights\n",
    "        layer.bias += -self.learning_rate*layer.dbias\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self,learning_rate=0.001,decay=0,epsilion=1e-7,beta_1=0.9,beta_2=0.9999):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.decay=decay\n",
    "        self.epsilion=epsilion\n",
    "        self.beta_1=beta_1\n",
    "        self.beta_2=beta_2\n",
    "        self.iterations=0\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate=self.learning_rate*(1/(1+self.decay*self.iterations))\n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_cache=np.zeros_like(layer.weights)\n",
    "            layer.weight_momentum=np.zeros_like(layer.weights)\n",
    "            layer.bias_cache=np.zeros_like(layer.bias)\n",
    "            layer.bias_momentum=np.zeros_like(layer.bias)\n",
    "        layer.weight_momentum=self.beta_1*layer.weight_momentum +(1-self.beta_1)*layer.dweights\n",
    "        layer.bias_momentum=self.beta_1*layer.bias_momentum + (1-self.beta_1)*layer.dbias\n",
    "        \n",
    "        weight_momentum_corrected=layer.weight_momentum / (1-self.beta_1**(self.iterations+1))\n",
    "        bias_momentum_corrected=layer.bias_momentum /(1-self.beta_1**(self.iterations+1))\n",
    "\n",
    "        layer.weight_cache=self.beta_2*layer.weight_cache +(1-self.beta_2)*layer.dweights**2\n",
    "        layer.bias_cache=self.beta_2*layer.bias_cache +(1-self.beta_2)*layer.dbias**2\n",
    "\n",
    "        weight_cache_corrected=layer.weight_cache/(1-self.beta_2**(self.iterations+1))\n",
    "        bias_cache_corrected=layer.bias_cache/(1-self.beta_2**(self.iterations+1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate*weight_momentum_corrected \\\n",
    "                         /(np.sqrt(weight_cache_corrected + self.epsilion))\n",
    "        layer.bias += -self.current_learning_rate*bias_momentum_corrected \\\n",
    "                     / (np.sqrt(bias_cache_corrected + self.epsilion))\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c22212a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, acc: 0.394, loss: 2.727\n",
      "epoch: 2, acc: 0.294, loss: 2.675\n",
      "epoch: 3, acc: 0.460, loss: 1.571\n",
      "epoch: 4, acc: 0.625, loss: 1.170\n",
      "epoch: 5, acc: 0.696, loss: 0.956\n",
      "epoch: 6, acc: 0.690, loss: 0.953\n",
      "epoch: 7, acc: 0.693, loss: 0.906\n",
      "epoch: 8, acc: 0.759, loss: 0.740\n",
      "epoch: 9, acc: 0.821, loss: 0.598\n",
      "epoch: 10, acc: 0.846, loss: 0.537\n",
      "epoch: 11, acc: 0.836, loss: 0.545\n",
      "epoch: 12, acc: 0.836, loss: 0.538\n",
      "epoch: 13, acc: 0.853, loss: 0.495\n",
      "epoch: 14, acc: 0.867, loss: 0.450\n",
      "epoch: 15, acc: 0.878, loss: 0.415\n",
      "epoch: 16, acc: 0.885, loss: 0.395\n",
      "epoch: 17, acc: 0.886, loss: 0.389\n",
      "epoch: 18, acc: 0.887, loss: 0.381\n",
      "epoch: 19, acc: 0.892, loss: 0.362\n",
      "epoch: 20, acc: 0.898, loss: 0.342\n",
      "epoch: 21, acc: 0.902, loss: 0.329\n",
      "epoch: 22, acc: 0.906, loss: 0.317\n",
      "epoch: 23, acc: 0.909, loss: 0.307\n",
      "epoch: 24, acc: 0.911, loss: 0.301\n",
      "epoch: 25, acc: 0.912, loss: 0.297\n",
      "epoch: 26, acc: 0.914, loss: 0.287\n",
      "epoch: 27, acc: 0.918, loss: 0.275\n",
      "epoch: 28, acc: 0.920, loss: 0.266\n",
      "epoch: 29, acc: 0.921, loss: 0.259\n",
      "epoch: 30, acc: 0.923, loss: 0.254\n",
      "epoch: 31, acc: 0.924, loss: 0.248\n",
      "epoch: 32, acc: 0.926, loss: 0.242\n",
      "epoch: 33, acc: 0.928, loss: 0.236\n",
      "epoch: 34, acc: 0.930, loss: 0.230\n",
      "epoch: 35, acc: 0.933, loss: 0.224\n",
      "epoch: 36, acc: 0.934, loss: 0.218\n",
      "epoch: 37, acc: 0.936, loss: 0.214\n",
      "epoch: 38, acc: 0.937, loss: 0.210\n",
      "epoch: 39, acc: 0.939, loss: 0.206\n",
      "epoch: 40, acc: 0.940, loss: 0.202\n",
      "epoch: 41, acc: 0.941, loss: 0.198\n",
      "epoch: 42, acc: 0.942, loss: 0.194\n",
      "epoch: 43, acc: 0.943, loss: 0.190\n",
      "epoch: 44, acc: 0.945, loss: 0.186\n",
      "epoch: 45, acc: 0.946, loss: 0.183\n",
      "epoch: 46, acc: 0.947, loss: 0.180\n",
      "epoch: 47, acc: 0.948, loss: 0.177\n",
      "epoch: 48, acc: 0.948, loss: 0.175\n",
      "epoch: 49, acc: 0.949, loss: 0.172\n"
     ]
    }
   ],
   "source": [
    "dense1=Layer_Dense(784,128)\n",
    "activation1=Activation_RelU()\n",
    "dense2=Layer_Dense(128,10)\n",
    "loss_activation=Activation_SoftMax_Loss_Catorgorical_Cross_Entropy()\n",
    "optimizer=Optimizer_Adam(learning_rate=0.02,decay=1e-5)\n",
    "for epoch in range(50):\n",
    "    dense1.forward(X_train)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss=loss_activation.forward(dense2.output,y_train)\n",
    "\n",
    "    predictions=np.argmax(loss_activation.output,axis=1)\n",
    "    accuracy=np.mean(predictions==y_train)\n",
    "    if not epoch %100==0:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy :.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "    loss_activation.backward(loss_activation.output,y_train)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f32ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validations Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7040c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.938, loss: 0.207\n"
     ]
    }
   ],
   "source": [
    "dense1.forward(X_val)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss=loss_activation.forward(dense2.output,y_val)\n",
    "\n",
    "predictions=np.argmax(loss_activation.output,axis=1)\n",
    "accuracy=np.mean(predictions==y_val)\n",
    "    \n",
    "print(f'acc: {accuracy :.3f}, ' +  f'loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45c5ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c6e38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.941, loss: 9.111\n"
     ]
    }
   ],
   "source": [
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss=loss_activation.forward(dense2.output,y_val)\n",
    "\n",
    "predictions=np.argmax(loss_activation.output,axis=1)\n",
    "accuracy=np.mean(predictions==y_test)\n",
    "    \n",
    "print(f'acc: {accuracy :.3f}, ' +  f'loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
